Network:
  label: VNet
  in_channels: 2
  out_channels: 3
  need_bias: True
  deep_supervision: False
  need_features: False
  features: [16, 32, 64, 128, 256, 512]
  dropout_p: [0., 0., 0., 0.3, 0.4, 0.5]
  num_conv_per_stage: [2, 2, 2, 2, 2, 2]
  kernel_size: [[1, 3, 3], [1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]
  pool_kernel_size: [[1, 2, 2], [1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]]
  normalization: InstanceNorm
  activate: leakyrelu

Training_settings:
  dataset_name: SegRap2023
  # modality of dataset, 
  # one modality: number 0, 1, 2, ...
  # some modalities but not all: list [0, 1, 2]
  # all the modalities: None or str "all"
  modality: all
  # to get the logs save folder's name easier to understand. If there is no content here, use time to name the folder
  method_name: Fully
  # gpu or cpu used for training
  device: {"gpu": [0]}
  # maximum epoch number to train
  epoch: 200
  # iter number to train per epoch
  tr_iterations_per_epoch: 250
  # iter number to train per epoch
  val_iterations_per_epoch: 50
  # batch_size per gpu
  batch_size: 2
  # patch size of network input in z, y, x
  patch_size: [40, 160, 288]
  # settings of optimizer
  base_lr: 0.01
  weight_decay: 3.0e-5
  # number of workers
  num_processes: 16
  # whether use deterministic training
  deterministic: True
  # random seed
  seed: 319
  # the proportion of patches sampled from foreground voxels in each batch
  oversample_rate: 0.33
  probabilistic_oversampling: False
  # ignore some classes when training
  ignore_label: 
  # path of model's weight to load to continue the training process. There can be no content here.
  checkpoint: 
  # path of pretrained weight to load. There can be no content here.
  pretrained_weight:
  